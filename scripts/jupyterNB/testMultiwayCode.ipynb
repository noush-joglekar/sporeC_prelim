{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import warnings\n",
    "import argparse\n",
    "import os\n",
    "import json\n",
    "\n",
    "import sys\n",
    "sys.path.append('/gpfs/commons/groups/gursoy_lab/ajoglekar/Projects/2023_03_01_multiwayInteractions/v0.analysis/scripts/pythonScripts/functions/')\n",
    "# from multiwayExpectedProbs import multiwayEval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDir = '/gpfs/commons/groups/gursoy_lab/ajoglekar/Projects/2023_03_01_multiwayInteractions/2023_03_01_v0_dataGathering/v0_hypergraphSimulations/getMultiwayInteractions_fromBPChains/'\n",
    "runDir = 'v2.evaluateExpectedVersusInteresting_sample2/'\n",
    "plotDir = f'{dataDir}{runDir}Plots_sample2_10kChains_500_750_1/'\n",
    "outDir = f'{dataDir}{runDir}dfs_sample2_10kChains_500_750_1/'\n",
    "inputPkl = 'v2.makeCombinedHypergraphDicts/sample2/hyperEdges_1_500_750_final_chains.pkl'\n",
    "probHashOutName = f'{outDir}probHash_10kChains_1_500_750.json'\n",
    "\n",
    "seed = 1\n",
    "quartile = 25\n",
    "toChoose = 2000\n",
    "toPlotRef = False\n",
    "toPlotInd = False\n",
    "toPlotScatter = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading in hyperedge pickle file\n",
      "Processing all the hyperedges from pickle file\n",
      "A total of 5371539 initial interactions\n",
      "Updated the input: retaining 1928433 interactions with chain support of at least 2\n",
      "Set up instance\n",
      "Expected probabilities file already exists...moving on\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading in hyperedge pickle file\")\n",
    "with open(f'{dataDir}/{inputPkl}','rb') as f:\n",
    "    hpEdges = pickle.load(f)\n",
    "\n",
    "    \n",
    "print(\"Processing all the hyperedges from pickle file\")\n",
    "hpKeys = [k for k in hpEdges.keys()]\n",
    "print(\"A total of\",len(hpKeys),\"initial interactions\")\n",
    "#keyCard = [len(item) for item in hpKeys_split]\n",
    "\n",
    "readSupport = [v[0] for v in hpEdges.values()]\n",
    "chainSupport = [v[1] for v in hpEdges.values()]\n",
    "readCards = [v[3] for v in hpEdges.values()]\n",
    "\n",
    "atLeastTwoChains = [i for i,x in enumerate(chainSupport) if x >=2]\n",
    "updatedDict = {hpKeys[i]:readSupport[i] for i in atLeastTwoChains}\n",
    "\n",
    "hpKeys = [k for k in updatedDict.keys()]\n",
    "hpKeys_split = [k.split(\"_\") for k in updatedDict.keys()]\n",
    "keyCard = [readCards[i] for i in atLeastTwoChains]\n",
    "\n",
    "print(\"Updated the input: retaining\",len(hpKeys),\n",
    "        \"interactions with chain support of at least 2\")\n",
    "\n",
    "print(\"Set up instance\")\n",
    "\n",
    "evalInstance = multiwayEval(keyCard, updatedDict, hpKeys, hpKeys_split, seed,\n",
    "                                toChoose, toPlotRef, toPlotInd, toPlotScatter,\n",
    "                                quartile, plotDir,outDir)\n",
    "\n",
    "if os.path.exists(probHashOutName):\n",
    "    print(\"Expected probabilities file already exists...moving on\")\n",
    "    with open(probHashOutName,'r') as file:\n",
    "        tmpHash = json.load(file)\n",
    "        probHash = {key: {int(k): v for k, v in value.items()} \n",
    "                    for key, value in tmpHash.items()}\n",
    "else:\n",
    "    print(\"Expected probabilities file does not exist...creating now\")\n",
    "    probHash = evalInstance.makeAllReferenceHashDicts()\n",
    "    with open(probHashOutName,'w') as file:\n",
    "        json.dump(probHash,file)\n",
    "    print(\"File created...moving on\")\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating for card= 9\n",
      "There are  35 reads\n",
      "Calculating stats for 35 reads\n",
      "Writing output for wDist\n",
      "Writing output for cosineSim\n",
      "Writing output for eDist\n",
      "Writing output for empDist\n",
      "Calculating for card= 8\n",
      "There are  2160 reads\n",
      "Calculating stats for 2000 reads\n",
      "Writing output for wDist\n",
      "Writing output for cosineSim\n",
      "Writing output for eDist\n",
      "Writing output for empDist\n",
      "Calculating for card= 7\n",
      "There are  45537 reads\n",
      "Calculating stats for 2000 reads\n",
      "Writing output for wDist\n",
      "Writing output for cosineSim\n",
      "Writing output for eDist\n",
      "Writing output for empDist\n",
      "Calculating for card= 6\n",
      "There are  359810 reads\n",
      "Calculating stats for 2000 reads\n",
      "Writing output for wDist\n",
      "Writing output for cosineSim\n",
      "Writing output for eDist\n",
      "Writing output for empDist\n",
      "Calculating for card= 5\n",
      "There are  861096 reads\n",
      "Calculating stats for 2000 reads\n",
      "Writing output for wDist\n",
      "Writing output for cosineSim\n",
      "Writing output for eDist\n",
      "Writing output for empDist\n",
      "Calculating for card= 4\n",
      "There are  531533 reads\n",
      "Calculating stats for 2000 reads\n",
      "Writing output for wDist\n",
      "Writing output for cosineSim\n",
      "Writing output for eDist\n",
      "Writing output for empDist\n",
      "Calculating for card= 3\n",
      "There are  123312 reads\n",
      "Calculating stats for 2000 reads\n",
      "Writing output for wDist\n",
      "Writing output for cosineSim\n",
      "Writing output for eDist\n",
      "Writing output for empDist\n"
     ]
    }
   ],
   "source": [
    "evalInstance = multiwayEval(keyCard, updatedDict, hpKeys, hpKeys_split, seed,\n",
    "                                toChoose, toPlotRef, toPlotInd, toPlotScatter,\n",
    "                                quartile, plotDir,outDir)\n",
    "A = evalInstance.statsForAllReads(probHash)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing output\n"
     ]
    }
   ],
   "source": [
    "evalInstance.processConcatDFs(A[0],\"wDist\",False,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import statistics\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats import wasserstein_distance\n",
    "from scipy.stats import energy_distance\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "class multiwayEval:\n",
    "\n",
    "    def __init__(self, keyCard, hpEdges, hpKeys, hpKeys_split, seed, \n",
    "                 toChoose,toPlotRef, toPlotInd, toPlotScatter,\n",
    "                 quartile, plotDir,outDir):\n",
    "        self.keyCard = keyCard\n",
    "        self.hpEdges = hpEdges\n",
    "        self.hpKeys = hpKeys\n",
    "        self.hpKeys_split = hpKeys_split\n",
    "        self.seed = seed\n",
    "        self.toPlotRef = toPlotRef\n",
    "        self.toPlotInd = toPlotInd\n",
    "        self.toPlotScatter = toPlotScatter\n",
    "        self.toChoose = toChoose\n",
    "        self.qt = quartile\n",
    "        self.plotDir = plotDir\n",
    "        self.outDir = outDir\n",
    "\n",
    "    def getReadSupportStatsPerCard(self, ixList, card):\n",
    "        \"\"\"Get disribution of read support per card so as to impose cutoffs\n",
    "        in the end as to the trustworthiness of reads\"\"\"\n",
    "        readSupps = [self.hpEdges[self.hpKeys[ix]] for ix in ixList]\n",
    "        rsStats = pd.Series(readSupps).describe()\n",
    "        rsStats.to_pickle(f'{self.outDir}readStats_card{card}.pkl')\n",
    "        return\n",
    "\n",
    "    def makeAllReferenceHashDicts(self):\n",
    "        \"\"\"For all available cards, create look up table of probability of\n",
    "        seeing all possible subsets as a function of mean distances\n",
    "        between subsets\"\"\"\n",
    "        probHash = defaultdict(dict)\n",
    "\n",
    "        for card in range(max(self.keyCard),2,-1):\n",
    "            print(\"Calculating for card=\",card)\n",
    "            ixList = [index for index,element in enumerate(self.keyCard) if element == card]\n",
    "            print(\"There are \",len(ixList),\"reads\")\n",
    "            self.getReadSupportStatsPerCard(ixList,card)\n",
    "            for n in range(2,card):\n",
    "                print(f\"Creating a probability hash for {n}-way subsets\")\n",
    "                hashID = f'{card}sub{n}'\n",
    "                probHash[hashID] = self.getNWayProbsPerCard(card,n)\n",
    "        return(probHash)\n",
    "\n",
    "    def getNWayProbsPerCard(self,card,n):\n",
    "        \"\"\"Get distance-stratified expected probability distribution for high\n",
    "        cardinality reads and their subsets of cardinality n\"\"\"\n",
    "        ixList = [index for index,element in enumerate(self.keyCard) if element == card]\n",
    "        mainDict = defaultdict(int)\n",
    "\n",
    "        for ix in ixList:\n",
    "            readSupport, nWayDict = self.makeNWayDict(ix,n)\n",
    "            for key in nWayDict.keys():\n",
    "                mainDict[key] += nWayDict[key]\n",
    "\n",
    "        normalized_values = self.getProbabilitiesByDist(mainDict)\n",
    "        if self.toPlotRef is True:\n",
    "            self.plotReadFreqsPerCard(mainDict,normalized_values,card,n)\n",
    "        \n",
    "        probabilityDict = {list(mainDict.keys())[ix]:normalized_values[ix] for ix in range(len(normalized_values))}\n",
    "        return(probabilityDict)\n",
    "    \n",
    "    def makeNWayDict(self,ix,n): ### Breaks down when filtered\n",
    "        \"\"\"For a high cardinality read, make a dictionary\n",
    "        that outputs the frequency and mean distance for subsets\n",
    "        of cardinality n\"\"\"\n",
    "        readSupport = self.hpEdges[self.hpKeys[ix]]\n",
    "        splitKey = self.hpKeys_split[ix]\n",
    "        combs = list(combinations(splitKey,n))\n",
    "        subsetDict = defaultdict(int)\n",
    "        for comb in combs:\n",
    "            subsetEdge = '_'.join(map(str, comb))\n",
    "            try:\n",
    "                subsetEdgeReads = self.hpEdges[subsetEdge]\n",
    "            except KeyError:\n",
    "                subsetEdgeReads = 0\n",
    "            meanDist = self.getNWayMeanDistPerSubset(comb)\n",
    "            subsetDict[meanDist] = subsetEdgeReads\n",
    "        return(readSupport, subsetDict)\n",
    "    \n",
    "    def getNWayMeanDistPerSubset(self,comb):\n",
    "        \"\"\"Get the mean pairwise distance given a\n",
    "        high cardinality read\"\"\"\n",
    "        twoWays = list(combinations(comb,2))\n",
    "        twoWayDist = [self.getPairwiseDist(c) for c in twoWays]\n",
    "        # meanDist = round(statistics.mean(twoWayDist))\n",
    "        meanDist = round(statistics.geometric_mean(twoWayDist)) ## geometric mean\n",
    "        return(meanDist)\n",
    "    \n",
    "    def getPairwiseDist(self,combination):\n",
    "        \"\"\"Given a two-way interaction, find dist between them\"\"\"\n",
    "        ends = [int(combination[i].split(\":\")[1]) for i in [0,1]]\n",
    "        diff = (ends[1] - ends[0]) / 5\n",
    "        return(diff)\n",
    "    \n",
    "    def getProbabilitiesByDist(self,mainDict):\n",
    "        \"\"\"Get probabilities of reads occurring by mean dist to\n",
    "        obtain the expected probability distribution from the data\"\"\"\n",
    "        totalReadsForCard = sum(mainDict.values())\n",
    "        if totalReadsForCard == 0:\n",
    "            return None\n",
    "        else:\n",
    "            normalized_values = [value / totalReadsForCard for value in mainDict.values()]\n",
    "            ## return total reads as well\n",
    "            return(normalized_values)\n",
    "    \n",
    "    def plotReadFreqsPerCard(self,mainDict,normalized_values,card,n):\n",
    "        \"\"\"Takes in a generated dictionary and plots the\n",
    "        read frequency as well as probabilities (first 20) by the\n",
    "        distance\"\"\"\n",
    "\n",
    "        # Step 1: Plot a barplot with dictionary keys on the x-axis and values on the y-axis\n",
    "        sns.barplot(x=list(mainDict.keys()), y=list(mainDict.values()),color = \"grey\")\n",
    "        plt.title(f\"Read frequency of {n}-way subsets for Card={card}\")\n",
    "        plt.xlabel(f\"Mean (Geom) distance between {n}-way interactions\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.xticks(rotation=90,fontsize=5)\n",
    "        plt.savefig(f'{self.plotDir}DistStratProbs_Card{card}sub{n}.png',bbox_inches = 'tight',facecolor = \"white\")\n",
    "        #plt.show()\n",
    "\n",
    "        # Step 2: Plot the same barplot with the ratio of each value to the total\n",
    "        sns.barplot(x=list(mainDict.keys()), y=normalized_values,color = \"grey\")\n",
    "        plt.title(f\"Probability of occurrence {n}-way distance for Card={card}\")\n",
    "        plt.xlabel(f\"Mean (Geom) distance between {n}-way interactions\")\n",
    "        plt.ylabel(\"Probabilities\")\n",
    "        plt.xlim(0,19)\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.savefig(f'{self.plotDir}DistStratProbs_top20_Card{card}sub{n}.png',bbox_inches = 'tight',facecolor = \"white\")\n",
    "        #plt.show()\n",
    "        plt.close()\n",
    "        return None\n",
    "    \n",
    "    def statsForAllReads(self,probHash):\n",
    "        \"\"\"Wrapper for all reads\"\"\"\n",
    "        for card in range(max(self.keyCard),3,-1):\n",
    "            allStats = self.statsForAllCardSubsets(card,probHash)\n",
    "            self.processConcatDFs(allStats[0],\"wDist\",False,card)\n",
    "            self.processConcatDFs(allStats[1],\"cosineSim\",True,card)\n",
    "            self.processConcatDFs(allStats[2],\"eDist\",False,card)\n",
    "            self.processConcatDFs(allStats[3],\"empDist\",True,card)\n",
    "        for card in [3]:\n",
    "            allStats = self.statsForAllCardSubsets(card,probHash)\n",
    "            self.processConcatDFs(allStats[0],\"wDist\",False,card)\n",
    "            self.processConcatDFs(allStats[1],\"cosineSim\",True,card)\n",
    "            self.processConcatDFs(allStats[2],\"eDist\",False,card)\n",
    "            self.processConcatDFs(allStats[3],\"empDist\",True,card)            \n",
    "        return None\n",
    "\n",
    "    def processConcatDFs(self,metricDF,metricName,simiBool,card):\n",
    "        \"\"\"summarize and write output for each metric\"\"\"\n",
    "        if simiBool is True:\n",
    "            qtCutoff = self.qt\n",
    "            operator = \"leq\"\n",
    "        else:\n",
    "            qtCutoff = 100 - self.qt\n",
    "            operator = \"geq\"\n",
    "        if card == 3:\n",
    "            metricCutoff = pd.Series(metricDF['3Sub2']).describe()[f'{qtCutoff}%']\n",
    "            self.plotSimilarityHist(metricDF['3Sub2'],metricName,card)\n",
    "            if operator == \"leq\":\n",
    "                mStatus = [1 if x else 0 for x in (metricDF['3Sub2'] <= metricCutoff)]\n",
    "            else:\n",
    "                mStatus = [1 if x else 0 for x in (metricDF['3Sub2'] >= metricCutoff)]\n",
    "        else:\n",
    "            summaryMetric = metricDF.filter(like=\"Sub\").agg((np.mean,np.std),axis = 1)        \n",
    "            metricCutoff = self.getCutoff(summaryMetric,qtCutoff)\n",
    "            self.plotSimilarityHist(summaryMetric['mean'],metricName,card)\n",
    "            if operator == \"leq\":\n",
    "                mStatus = [1 if x else 0 for x in (summaryMetric['mean'] <= metricCutoff)]\n",
    "            else:\n",
    "                mStatus = [1 if x else 0 for x in (summaryMetric['mean'] >= metricCutoff)]\n",
    "            if self.toPlotScatter is True:\n",
    "                self.plotScatterWithErrorBars(summaryMetric,metricName,card)\n",
    "        metricDF['Status'] = mStatus\n",
    "        print(f\"Writing output for {metricName}\")\n",
    "        metricDF.to_csv(f'{self.outDir}/{metricName}_card{card}.csv',sep = \"\\t\",index=False)\n",
    "        return None\n",
    "    \n",
    "    def getCutoff(self,summaryDF,quartile):\n",
    "        q = f'{quartile}%'\n",
    "        cutoff = pd.Series(summaryDF['mean']).describe()[q]\n",
    "        return(cutoff)\n",
    "\n",
    "    def statsForAllCardSubsets(self,card,probHash):\n",
    "        \"\"\"Wrapper for the stats per card. Calculates for all subsets of a card and binds into a df\"\"\"\n",
    "        print(\"Calculating for card=\",card)\n",
    "        ixList = [index for index,element in enumerate(self.keyCard) if element == card]\n",
    "        print(\"There are \",len(ixList),\"reads\")\n",
    "        cardToChoose = min(self.toChoose,len(ixList))\n",
    "        print(\"Calculating stats for\",cardToChoose,\"reads\")\n",
    "        random.seed(self.seed)\n",
    "        revised_ixes = random.sample(ixList,cardToChoose)\n",
    "        C1 = []\n",
    "        C2 = []\n",
    "        C3 = []\n",
    "        C4 = []\n",
    "        for n in range(2,card):\n",
    "            stats = self.getStatsPerCard(card,n,probHash,revised_ixes)\n",
    "            C1.append(stats[0])\n",
    "            C2.append(stats[1])\n",
    "            C3.append(stats[2])\n",
    "            C4.append(stats[3])\n",
    "            if n == 2:\n",
    "                C5 = stats[4]\n",
    "        cN = [str(card)+\"Sub\"+str(i) for i in range(2,card)]\n",
    "        df1 = pd.DataFrame(C1).T\n",
    "        df2 = pd.DataFrame(C2).T\n",
    "        df3 = pd.DataFrame(C3).T\n",
    "        df4 = pd.DataFrame(C4).T\n",
    "        df1.columns = cN\n",
    "        df1['Edge_ix'] = revised_ixes\n",
    "        df1['ReadSupport'] = C5\n",
    "        df2.columns = cN\n",
    "        df2['Edge_ix'] = revised_ixes\n",
    "        df2['ReadSupport'] = C5\n",
    "        df3.columns = cN\n",
    "        df3['Edge_ix'] = revised_ixes\n",
    "        df3['ReadSupport'] = C5\n",
    "        df4.columns = cN\n",
    "        df4['Edge_ix'] = revised_ixes\n",
    "        df4['ReadSupport'] = C5\n",
    "        return(df1,df2,df3,df4)\n",
    "\n",
    "    def getStatsPerCard(self,card,n,probHash,revised_ixes):\n",
    "        \"\"\"Per cardinality, get a subset of reads (for computational\n",
    "        purposes), calculate the similarity of observed n-way interactions\n",
    "        to the expected value, and output a list\"\"\"\n",
    "        wdistList = []\n",
    "        cosList = []\n",
    "        edistList = []\n",
    "        empDistList = []\n",
    "        readSuppList = []\n",
    "        expHash = f'{card}sub{n}'\n",
    "        for ix in revised_ixes:\n",
    "            stats = self.getReadExpectednessStats(card,ix,n,probHash[expHash])\n",
    "            if stats is not None:\n",
    "                wdistList.append(stats[0])\n",
    "                cosList.append(stats[1])\n",
    "                edistList.append(stats[2])\n",
    "                empDistList.append(stats[3])\n",
    "                readSuppList.append(stats[4])\n",
    "        return(wdistList, cosList, edistList, empDistList, readSuppList)\n",
    "    \n",
    "    def getReadExpectednessStats(self,card,ix,n,hash):\n",
    "        \"\"\"Per read, get the observed distribution of n-way contacts\n",
    "        and calculate similarity to the expected distribution\"\"\"\n",
    "        readSupport, readDict = self.makeNWayDict(ix,n)\n",
    "        readPercs= self.getProbabilitiesByDist(readDict)\n",
    "        if readPercs is None:\n",
    "            return None\n",
    "        else:\n",
    "            probVals = [hash[k] for k in readDict.keys()]\n",
    "            normProbs = [p / sum(probVals) for p in probVals]\n",
    "            probVals = normProbs  ## Normalizing mostly for the empirical calculation\n",
    "            if self.toPlotInd is True:\n",
    "                if len(readPercs) > 2 and len(probVals) > 2:\n",
    "                    self.makeSanityCheckPlotsPerRead(readDict,readPercs,probVals,card,n,ix)\n",
    "            wDist = wasserstein_distance(readPercs, probVals)\n",
    "            similarity = cosine_similarity(np.array(readPercs).reshape(1,-1), \n",
    "                                        np.array(probVals).reshape(1,-1))[0,0]\n",
    "            eDist = energy_distance(readPercs, probVals)\n",
    "            empDist = self.calcEmpDist(readPercs, probVals)\n",
    "            return(wDist, similarity, eDist, empDist, readSupport)\n",
    "    \n",
    "    def calcEmpDist(self,readPercs, probVals):\n",
    "        \"\"\"Trying a different metric for distance. \n",
    "        We will figure out the cutoff later\"\"\"\n",
    "        obs = np.log(readPercs)\n",
    "        exp = np.log(probVals)\n",
    "        if 0 in exp:\n",
    "            meanOE = 1\n",
    "        else:\n",
    "            obsOverExp = obs/exp\n",
    "            meanOE = np.mean(obsOverExp[np.isfinite(obsOverExp)])\n",
    "        return(meanOE)\n",
    "\n",
    "    def makeSanityCheckPlotsPerRead(self,readDict, readPercs, probVals, card, n, ix):\n",
    "        \"\"\"For specific reads, plot the observed versus expected distributions\n",
    "        of two-way interactions along with a fitted spline. Additionally outputs\n",
    "        the slope (useful only if line) and wDist values\"\"\"\n",
    "        Distances = list(readDict.keys())\n",
    "        print(card,\"sub\",n)\n",
    "        print(ix)\n",
    "        #print(readPercs)\n",
    "        #print(probVals)\n",
    "        \n",
    "        # Create a 2x2 grid of subplots\n",
    "        fig, axs = plt.subplots(2, 2, figsize=(6, 6))\n",
    "        # Plot 1: Observed w/ spline\n",
    "        lowess1 = sns.regplot(x=Distances, y=readPercs, \n",
    "                              lowess=True, ci=None, color='red', ax=axs[0, 0])\n",
    "        axs[0, 0].set_title(f\"Observed w/ spline Card{card}sub{n}\")\n",
    "        # Plot 2: Expected w/ spline\n",
    "        lowess2 = sns.regplot(x=Distances, y=probVals, \n",
    "                              lowess=True, ci=None, color='grey', ax=axs[0, 1])\n",
    "        axs[0, 1].set_title(f\"Expected w/ spline Card{card}sub{n}\")\n",
    "\n",
    "        # Calculate smoothed values and slopes\n",
    "        y_smoothed1 = lowess1.get_lines()[0].get_ydata()\n",
    "        y_smoothed2 = lowess2.get_lines()[0].get_ydata()\n",
    "        slope1 = np.gradient(y_smoothed1)\n",
    "        slope2 = np.gradient(y_smoothed2)\n",
    "\n",
    "        # Plot 3: Barplot for readPercs\n",
    "        sns.barplot(x=list(readDict.keys()), y=readPercs, ax=axs[1, 0])\n",
    "        axs[1, 0].set_title(f\"Barplot for readPercs Card{card}sub{n}\")\n",
    "        # Plot 4: Barplot for probVals\n",
    "        sns.barplot(x=list(readDict.keys()), y=probVals, ax=axs[1, 1])\n",
    "        axs[1, 1].set_title(f\"Barplot for normalized prior probs Card{card}sub{n}\")\n",
    "\n",
    "        #correlation = np.corrcoef(readPercs, probVals)[0, 1]\n",
    "        wDist = wasserstein_distance(readPercs, probVals)\n",
    "        similarity = cosine_similarity(np.array(readPercs).reshape(1,-1), \n",
    "                    np.array(probVals).reshape(1,-1))[0,0]\n",
    "        eDist = energy_distance(readPercs, probVals)\n",
    "        empDist = self.calcEmpDist(readPercs, probVals)\n",
    "        \n",
    "        # Print the slopes\n",
    "        print(\"Comparing -------------\")\n",
    "        print(f\"Slope for observed: {slope1.mean()}\")\n",
    "        print(f\"Slope for expected: {slope2.mean()}\")\n",
    "        print(f\"Wasserstein distance: {wDist}\")\n",
    "        print(f\"Energy distance: {eDist}\")\n",
    "        print(f\"Empirical distance: {empDist}\")\n",
    "        print(f\"Cosine similarity: {similarity}\")\n",
    "\n",
    "        # Adjust layout and show the subplots\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{self.plotDir}/SingleReadPlot_Card{card}sub{n}_{ix}.png',bbox_inches = 'tight',facecolor = \"white\")\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        return None\n",
    "\n",
    "    def plotSimilarityHist(self,metricDistr,metricName,card):\n",
    "        \"\"\"Given distribution for a cardinality, plot the wass dist\n",
    "        and cosine similarity values so that we can settle on a heuristic\"\"\"\n",
    "        plt.figure(figsize=(4,4))\n",
    "        plt.hist(metricDistr,color='blue', alpha=0.7, width=0.3,bins = 201)\n",
    "        plt.xlim(-1,1.01)\n",
    "        plt.title(f\"Mean {metricName} distance for card={card}\")\n",
    "        plt.close()\n",
    "        return None\n",
    "\n",
    "    def plotScatterWithErrorBars(self,summaryDF,metric,card):\n",
    "        \"\"\"Given summary stats for a certain number of reads, plot scatter plot with error bars \n",
    "        representing change over all subsets\"\"\"\n",
    "        sortedSumm = summaryDF.sort_values(by='mean')\n",
    "        x = [i+1 for i in range(summaryDF.shape[0])] # Use the index as x-axis\n",
    "        y = sortedSumm['mean']\n",
    "        error = sortedSumm['std']\n",
    "\n",
    "        plt.scatter(x, y, label=f'mean {metric}', marker='o')\n",
    "        plt.errorbar(x, y, yerr=error, linestyle='None', color='grey', capsize=3)\n",
    "        plt.xlabel('ReadID')\n",
    "        plt.ylabel(f'mean {metric}')\n",
    "        plt.ylim(0,2)\n",
    "        plt.title(f'Distribution for card={card}')\n",
    "        plt.legend()\n",
    "        plt.savefig(f'{self.plotDir}/ScatterPlot_{metric}_Card{card}_max{self.toChoose}reads.png',\n",
    "                    bbox_inches = 'tight',facecolor = \"white\")\n",
    "        #plt.show()\n",
    "        plt.close()\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hypergraph_poreC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
